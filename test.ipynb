{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba as MambaLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, input_dim * 2)\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out[:, :, :x.shape[2]] * torch.sigmoid(out[:, :, x.shape[2]:])\n",
    "    \n",
    "class MambaBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, state_dim, conv_dim, expansion, dropout, prenorm):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.mamba = MambaLayer(d_model=hidden_dim, d_state=state_dim, d_conv=conv_dim, expand=expansion)\n",
    "        self.glu = GLU(hidden_dim)\n",
    "        self.activation = torch.nn.GELU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.prenorm = prenorm\n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        if self.prenorm:\n",
    "            x = self.norm(x)\n",
    "        x = self.mamba(x)\n",
    "        x = self.dropout(self.activation(x))\n",
    "        x = self.glu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + skip\n",
    "        if not self.prenorm:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "    \n",
    "class Mamba(torch.nn.Module):\n",
    "    def __init__(self, num_blocks, input_dim, output_dim, hidden_dim, state_dim, conv_dim, expansion, dropout, prenorm):\n",
    "        super().__init__()\n",
    "        self.linear_encoder = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = torch.nn.Sequential(*[MambaBlock(hidden_dim, state_dim, conv_dim, expansion, dropout, prenorm) for _ in range(num_blocks)])\n",
    "        self.linear_decoder = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.linear_encoder(x)\n",
    "        x = self.blocks(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = torch.softmax(self.linear_decoder(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    seed,\n",
    "\tdataloader,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    wd,\n",
    "    num_blocks,\n",
    "\tinput_dim,\n",
    "\toutput_dim,\n",
    "    hidden_dim,\n",
    "    state_dim,\n",
    "    conv_dim,\n",
    "    expansion,\n",
    "    dropout,\n",
    "    prenorm\n",
    "    ):\n",
    "    torch.manual_seed(seed)\n",
    "    device = \"cuda\"\n",
    "    model = Mamba(num_blocks, input_dim, output_dim, hidden_dim, state_dim, conv_dim, expansion, dropout, prenorm).to(device)\n",
    "    print(\"Nr. of parameters: {0}\".format(sum(p.numel() for p in model.parameters())))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        train_accuracy = 0.0\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            accuracy = (y_hat.argmax(dim=1) == y).float().sum() / len(y)\n",
    "            train_accuracy += accuracy\n",
    "        print(train_accuracy / len(dataloader))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train, val_split):\n",
    "    train_len = int(len(train) * (1.0-val_split))\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        train,\n",
    "        (train_len, len(train) - train_len),\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=122.6 / 255.0, std=61.0 / 255.0),\n",
    "        transforms.Lambda(lambda x: x.view(1, 1024).t())\n",
    "    ])\n",
    "\n",
    "# Train with no data augmentation\n",
    "transform_train = transform_test = transform\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar/', train=True, download=True, transform=transform_train)\n",
    "trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar/', train=True, download=True, transform=transform_test)\n",
    "_, valset = split_train_val(valset, val_split=0.1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar/', train=False, download=True, transform=transform_test)\n",
    "\n",
    "d_input = 1 \n",
    "d_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mamba(4, 1, 10, 64, 64, 4, 2).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of parameters: 800266\n",
      "tensor(0.2942, device='cuda:0')\n",
      "tensor(0.3396, device='cuda:0')\n",
      "tensor(0.4153, device='cuda:0')\n",
      "tensor(0.4485, device='cuda:0')\n",
      "tensor(0.4818, device='cuda:0')\n",
      "tensor(0.5144, device='cuda:0')\n",
      "tensor(0.5279, device='cuda:0')\n",
      "tensor(0.5146, device='cuda:0')\n",
      "tensor(0.5332, device='cuda:0')\n",
      "tensor(0.5583, device='cuda:0')\n",
      "tensor(0.5776, device='cuda:0')\n",
      "tensor(0.5827, device='cuda:0')\n",
      "tensor(0.5898, device='cuda:0')\n",
      "tensor(0.5572, device='cuda:0')\n",
      "tensor(0.5951, device='cuda:0')\n",
      "tensor(0.6089, device='cuda:0')\n",
      "tensor(0.6126, device='cuda:0')\n",
      "tensor(0.6128, device='cuda:0')\n",
      "tensor(0.6267, device='cuda:0')\n",
      "tensor(0.6299, device='cuda:0')\n",
      "tensor(0.6269, device='cuda:0')\n",
      "tensor(0.6428, device='cuda:0')\n",
      "tensor(0.6449, device='cuda:0')\n",
      "tensor(0.6505, device='cuda:0')\n",
      "tensor(0.6584, device='cuda:0')\n",
      "tensor(0.6437, device='cuda:0')\n",
      "tensor(0.6536, device='cuda:0')\n",
      "tensor(0.6336, device='cuda:0')\n",
      "tensor(0.6667, device='cuda:0')\n",
      "tensor(0.6415, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    seed=1234,\n",
    "\tdataloader=trainloader,\n",
    "\tnum_epochs=30,\n",
    "    learning_rate=2e-4,\n",
    "    wd=0.1,\n",
    "    num_blocks=6,\n",
    "\tinput_dim=1,\n",
    "\toutput_dim=10,\n",
    "    hidden_dim=64,\n",
    "    state_dim=256,\n",
    "    conv_dim=4,\n",
    "    expansion=2,\n",
    "    dropout=0.0,\n",
    "    prenorm=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
