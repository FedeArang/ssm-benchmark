seed: 1919
wandb:
  group: "Pathfinder"
  name: "lin-attention"
  key: "58d1b0b4e77ad3dd9ebd08eb490255e83aa70bfe"
  entity: "ssm-eth"
  project: "neurips-2024-new"
dataset:
  name: "Pathfinder-32"
  _name_: "pathfinder"
  resolution: 32
  tokenize: True
  data_dir: "/cluster/scratch/jsieber/pathfinder32"
train:
  num_epochs: 200
  batch_size: 64
  lr: 0.001
  wd: 0.0
  warmup: 20
model:
  # task specific dims
  input_dim: 1
  output_dim: 2
  # backbone model
  layer: "transformer"
  attention_fn: "sm-attention"
  num_layers: 1
  hidden_dim: 128
  state_dim: 64
  num_heads: 2
  att_dropout: 0.2
  norm: "layer"
  # embedding & positional embedding
  embedding: True
  vocab_size: 256
  max_pos_embed: 1024
  # mlp
  mlp_dim: 128
  # global dropout rate
  dropout: 0.1
  # classifier
  classifier: True
  pooling: "mean"
  # use dual classification
  dual: False
